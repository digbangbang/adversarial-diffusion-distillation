{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebc950b-e0cd-431a-9256-80a846abe5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - pyarrow==9.0.0\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install diffusers datasets transformers accelerate ftfy pyarrow==9.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc2731-f057-41de-9f8d-7cd03ddaa266",
   "metadata": {},
   "source": [
    "https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416e500-8fc9-49df-a523-a5a829910c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # the generated image resolution\n",
    "    batch_size = 32\n",
    "    num_epochs = 100\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "\n",
    "    device = \"cuda\"\n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46cc8c1f-113a-45bb-bfd3-8aa945d8412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_everything(seed: int,\n",
    "                    use_deterministic_algos: bool = False) -> None:\n",
    "    \n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(use_deterministic_algos)\n",
    "    random.seed(seed)\n",
    "    \n",
    "   \n",
    "seed_everything(config.random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0e963-7b38-4357-b44e-d506cf17d65f",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38434222-99e3-44aa-a1cb-dcc866adae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images(x):\n",
    "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
    "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
    "    grid = torchvision.utils.make_grid(x)\n",
    "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
    "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
    "    return grid_im\n",
    "    \n",
    "\n",
    "\n",
    "def make_grid(images, size=64):\n",
    "    \"\"\"Given a list of PIL images, stack them together into a line for easy viewing\"\"\"\n",
    "    output_im = Image.new(\"RGB\", (size * len(images), size))\n",
    "    for i, im in enumerate(images):\n",
    "        output_im.paste(im.resize((size, size)), (i * size, 0))\n",
    "    return output_im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f28fde-fc81-4dde-b678-2c5271cb5ce6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ad851f-af52-4825-b6c8-8de1159e1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset = load_dataset(\"food101\", split='train')\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images, \"label\": examples[\"label\"]}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=config.batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66b1ce8-ed3d-4d43-a89c-1b592f19d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8e30c-18f7-450f-a9b1-52b1156ef0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))[\"images\"][:1].repeat(8, 1, 1, 1)\n",
    "timesteps = torch.linspace(0, 999, 8).long()\n",
    "noise = torch.randn_like(x)\n",
    "noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "print(\"Noisy X shape\", noisy_x.shape)\n",
    "show_images(noisy_x).resize((8 * 128, 128), resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59886f49-5728-4a61-ad39-b8d9a3bdc47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db4589-14fb-4dd0-ba74-ec3789356ef9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ad9c3-9504-41e6-8d6e-842efce8dd6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "    num_class_embeds=102,\n",
    "    down_block_types=( \n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\", \n",
    "        \"DownBlock2D\", \n",
    "        \"DownBlock2D\", \n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ), \n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\"  \n",
    "      ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87447dcd-3934-4550-b234-5777499dec85",
   "metadata": {},
   "source": [
    "# Let's Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d16945f-01ee-4cbc-af89-902ef8b7232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987, 986,\n",
       "        985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974, 973, 972,\n",
       "        971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961, 960, 959, 958,\n",
       "        957, 956, 955, 954, 953, 952, 951, 950, 949, 948, 947, 946, 945, 944,\n",
       "        943, 942, 941, 940, 939, 938, 937, 936, 935, 934, 933, 932, 931, 930,\n",
       "        929, 928, 927, 926, 925, 924, 923, 922, 921, 920, 919, 918, 917, 916,\n",
       "        915, 914, 913, 912, 911, 910, 909, 908, 907, 906, 905, 904, 903, 902,\n",
       "        901, 900, 899, 898, 897, 896, 895, 894, 893, 892, 891, 890, 889, 888,\n",
       "        887, 886, 885, 884, 883, 882, 881, 880, 879, 878, 877, 876, 875, 874,\n",
       "        873, 872, 871, 870, 869, 868, 867, 866, 865, 864, 863, 862, 861, 860,\n",
       "        859, 858, 857, 856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846,\n",
       "        845, 844, 843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832,\n",
       "        831, 830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n",
       "        817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805, 804,\n",
       "        803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792, 791, 790,\n",
       "        789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779, 778, 777, 776,\n",
       "        775, 774, 773, 772, 771, 770, 769, 768, 767, 766, 765, 764, 763, 762,\n",
       "        761, 760, 759, 758, 757, 756, 755, 754, 753, 752, 751, 750, 749, 748,\n",
       "        747, 746, 745, 744, 743, 742, 741, 740, 739, 738, 737, 736, 735, 734,\n",
       "        733, 732, 731, 730, 729, 728, 727, 726, 725, 724, 723, 722, 721, 720,\n",
       "        719, 718, 717, 716, 715, 714, 713, 712, 711, 710, 709, 708, 707, 706,\n",
       "        705, 704, 703, 702, 701, 700, 699, 698, 697, 696, 695, 694, 693, 692,\n",
       "        691, 690, 689, 688, 687, 686, 685, 684, 683, 682, 681, 680, 679, 678,\n",
       "        677, 676, 675, 674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664,\n",
       "        663, 662, 661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650,\n",
       "        649, 648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n",
       "        635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623, 622,\n",
       "        621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610, 609, 608,\n",
       "        607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597, 596, 595, 594,\n",
       "        593, 592, 591, 590, 589, 588, 587, 586, 585, 584, 583, 582, 581, 580,\n",
       "        579, 578, 577, 576, 575, 574, 573, 572, 571, 570, 569, 568, 567, 566,\n",
       "        565, 564, 563, 562, 561, 560, 559, 558, 557, 556, 555, 554, 553, 552,\n",
       "        551, 550, 549, 548, 547, 546, 545, 544, 543, 542, 541, 540, 539, 538,\n",
       "        537, 536, 535, 534, 533, 532, 531, 530, 529, 528, 527, 526, 525, 524,\n",
       "        523, 522, 521, 520, 519, 518, 517, 516, 515, 514, 513, 512, 511, 510,\n",
       "        509, 508, 507, 506, 505, 504, 503, 502, 501, 500, 499, 498, 497, 496,\n",
       "        495, 494, 493, 492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482,\n",
       "        481, 480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468,\n",
       "        467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n",
       "        453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441, 440,\n",
       "        439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428, 427, 426,\n",
       "        425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412,\n",
       "        411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398,\n",
       "        397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384,\n",
       "        383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 372, 371, 370,\n",
       "        369, 368, 367, 366, 365, 364, 363, 362, 361, 360, 359, 358, 357, 356,\n",
       "        355, 354, 353, 352, 351, 350, 349, 348, 347, 346, 345, 344, 343, 342,\n",
       "        341, 340, 339, 338, 337, 336, 335, 334, 333, 332, 331, 330, 329, 328,\n",
       "        327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314,\n",
       "        313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300,\n",
       "        299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286,\n",
       "        285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n",
       "        271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258,\n",
       "        257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244,\n",
       "        243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230,\n",
       "        229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216,\n",
       "        215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202,\n",
       "        201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188,\n",
       "        187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174,\n",
       "        173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160,\n",
       "        159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146,\n",
       "        145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132,\n",
       "        131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118,\n",
       "        117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104,\n",
       "        103, 102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n",
       "         89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,  76,\n",
       "         75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,  63,  62,\n",
       "         61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,  50,  49,  48,\n",
       "         47,  46,  45,  44,  43,  42,  41,  40,  39,  38,  37,  36,  35,  34,\n",
       "         33,  32,  31,  30,  29,  28,  27,  26,  25,  24,  23,  22,  21,  20,\n",
       "         19,  18,  17,  16,  15,  14,  13,  12,  11,  10,   9,   8,   7,   6,\n",
       "          5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, \n",
    "    beta_schedule=\"linear\"\n",
    ")\n",
    "\n",
    "noise_scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "noise_scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfd5bec-77ab-48c3-b2a0-c5c69f261eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazarev/miniconda3/envs/ftcp/lib/python3.11/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_scheduler.num_inference_steps, noise_scheduler.num_train_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c061bc50-e93e-4784-861d-35c35d66be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f22c9e5-7546-42c2-bdbb-039a81f52558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    ")\n",
    "\n",
    "train_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, model, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8300a4-8b45-4b57-ae1b-ff8c873be231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(x, model, noise_scheduler, num_inference_steps: int = 1000):\n",
    "    model.eval()\n",
    "\n",
    "    bs = x.shape[0]\n",
    "\n",
    "    y = torch.randint(\n",
    "            0, 102, (bs,), device=config.device\n",
    "        ).long()\n",
    "    \n",
    "    noise_scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "\n",
    "    for t in tqdm(noise_scheduler.timesteps):\n",
    "        model_input = noise_scheduler.scale_model_input(x, t)\n",
    "\n",
    "        t_batch = torch.full(\n",
    "            size=(bs,), \n",
    "            fill_value=t.item(), \n",
    "            dtype=torch.long\n",
    "        ).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(\n",
    "                model_input, \n",
    "                t_batch, \n",
    "                y,\n",
    "                return_dict=False\n",
    "            )[0]\n",
    "\n",
    "        x = noise_scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c2fe269-8bfd-417d-9b74-75b483d1db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zero_class(y):\n",
    "    bs = y.shape[0]\n",
    "\n",
    "    y = (y + 1) * (torch.rand((bs,)) >= 0.1).long().to(y.device)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987be5c7-b268-4858-891a-926181099949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe892b5655346bca064584e16d14a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(config.device)\n",
    "        labels = add_zero_class(batch[\"label\"]).to(config.device)\n",
    "\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(config.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=config.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "                    \n",
    "        with accelerator.accumulate(model):\n",
    "            noise_pred = model(\n",
    "                noisy_images, \n",
    "                timesteps, \n",
    "                labels,\n",
    "                return_dict=False\n",
    "            )[0]\n",
    "    \n",
    "            # Calculate the loss\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "            # Update the model parameters with the optimizer\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "        print(f\"Epoch:{epoch + 1}, loss: {loss_last_epoch}\")\n",
    "\n",
    "        generated = generate(\n",
    "            torch.randn((16, 3, 128, 128)).cuda(), \n",
    "            model, \n",
    "            noise_scheduler,\n",
    "            num_inference_steps=100\n",
    "        ).cpu()\n",
    "        pil_images = show_images(generated)\n",
    "        plt.imshow(pil_images)\n",
    "\n",
    "        torch.save(model, f\"food101_conditional_ddpm/model_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0c41f-b024-4315-8191-4a4630ccc577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
